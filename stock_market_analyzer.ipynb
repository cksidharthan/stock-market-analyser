{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pycurl as pycurl\n",
    "from io import BytesIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './html_files'\n"
     ]
    }
   ],
   "source": [
    "# Create new folder\n",
    "try:  \n",
    "    os.mkdir('./html_files')  \n",
    "except OSError as error:  \n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the files in the html_files folder\n",
    "file_path = './html_files/'\n",
    "file_list = os.listdir('./html_files')\n",
    "for file in file_list:\n",
    "    os.remove(file_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Number of companies to analyse: 2\n",
      "Enter the Stock name of the company: AAPL\n",
      "Enter the Stock name of the company: AMZN\n"
     ]
    }
   ],
   "source": [
    "# Get values from Users\n",
    "company_list = []\n",
    "url_list = []\n",
    "finance_url = 'https://finviz.com/quote.ashx?t='\n",
    "num_companies = int(input('Enter the Number of companies to analyse: '))\n",
    "for company in range(num_companies):\n",
    "    company_stock_name = input('Enter the Stock name of the company: ')\n",
    "    company_list.append(company_stock_name.upper())\n",
    "    url_list.append(finance_url +company_stock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website Downloader\n",
    "def url_downloader(url):\n",
    "    byte_obj = BytesIO() \n",
    "    curl = pycurl.Curl() \n",
    "    curl.setopt(curl.URL, url)\n",
    "    curl.setopt(curl.WRITEDATA, byte_obj)\n",
    "    curl.perform() \n",
    "    curl.close()\n",
    "    # Get the content stored in the BytesIO object (in byte characters) \n",
    "    get_body = byte_obj.getvalue()\n",
    "    # Decode the bytes stored in get_body to HTML and print the result \n",
    "    return get_body.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write website html to file\n",
    "def write_to_file(file_name, file_data):\n",
    "    f = open( file_path + file_name + ''.html', 'w')\n",
    "    f.write(file_data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over company list and download the website html\n",
    "for company_index in range(num_companies):\n",
    "    try:\n",
    "        website_cache = url_downloader(url_list[company_index])\n",
    "        write_to_file(company_list[company_index], website_cache)\n",
    "    except:\n",
    "        raise Exception('Website not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files from html_files folder\n",
    "# data dictionary\n",
    "html_pages = {}\n",
    "\n",
    "for page_name in os.listdir('./html_files'):\n",
    "    page_path = f'./html_files/{page_name}'\n",
    "    page_file = open(page_path, 'r')\n",
    "    html_data = BeautifulSoup(page_file)\n",
    "    html_page = html_data.find(id = 'news-table')\n",
    "    html_pages[page_name] = html_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Hyperlinks and headlines \n",
    "def get_hyperlinks_headlines(company_data):\n",
    "    tr_data = company_data.findAll('tr')\n",
    "    for index, tr in enumerate(tr_data):\n",
    "        hyperlinks = tr_data.a.get_text()\n",
    "        headlines = tr_data.td.get_text()\n",
    "        print(f'{index} | {hyperlinks} | {headlines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
